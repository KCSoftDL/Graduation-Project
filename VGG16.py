import time
import os
import numpy as np
import tensorflow as tf
import tensorflow.keras as keras
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import TensorBoard,ReduceLROnPlateau,ModelCheckpoint
from tensorflow.keras.applications.vgg16 import preprocess_input,decode_predictions
from Datasets_loader import load_data_by_keras
from tensorflow.keras.preprocessing import image as keras_image
from util import read_chinesefoodnet_from_xlsx
import Datasets_loader


def ClassificationLayer(base_model, nb_classes):
    '''
    添加分类层
    :param base_model:
    :param nb_classes:
    :return:
    '''
    global_average_layer = keras.layers.GlobalAveragePooling2D(name='avg_pool')
    flatten_layer = keras.layers.Flatten(name='flatten')
    prediction_layer = keras.layers.Dense(nb_classes, activation='softmax')
    model = keras.Sequential([base_model,
                              global_average_layer,
                              flatten_layer,
                              prediction_layer
    ],name="My_VGG16")
    return model

logs = {'loss': [6.023179337218568, 5.24001727785383, 5.024234939407516, 4.822357513092377, 4.646031960526197, 4.538959621073126, 4.509010527160141, 4.447198485280131, 4.3936827051770555, 4.4470663620875435, 4.248756020933717, 4.381840760891254, 4.263336519618611, 4.139666305793511, 4.154553573210161, 4.133541628554627, 4.16502891268049, 4.01092066345634, 4.034262209148197, 4.114722610829951, 4.249262065677852, 4.112904551265004, 3.9584895437890357, 4.091869375207922, 3.9796629633222307, 4.03715046945509, 4.037606862875132, 3.9373745865874237, 4.008763525512192, 4.010959054087545, 4.065991105614128, 3.9249798832358893, 3.7896267555572174, 4.049577699912773, 3.8772775655264384, 3.9217822682726515, 3.916193330680931, 3.8917008756281257, 3.4034784206977258, 3.357393796627338, 3.3371463948553735, 3.29602541242327, 3.3822992660187103, 3.367596618421785, 3.370428795342917, 3.331401217114794, 3.3110159596244055, 3.2329825306986715, 3.285111516386598, 3.2228946738190705, 3.342202718441303, 3.241151458614475, 3.2674713815961565, 3.1281462134895746, 3.2578583418667972, 3.292475558899261, 3.1938459427802117, 3.0979667207696937, 3.207879540684459, 3.199509096669627, 3.1973233196761583, 3.131290708269392, 3.2434704906337863, 3.1927428586142406, 3.0904699786678775, 3.1160174097333635, 3.1948613098689487, 3.2246389598636838, 3.2509863586216183, 3.153476670548156, 3.1932780061449324, 3.102143773665795, 3.1756230055630863, 3.2143026131850023, 3.2102753461062252, 3.1016665736397546, 3.1277961416558906, 3.1587966746026344, 3.0671729294808356, 3.1350190744295223, 3.1223784462436215, 3.0848639771178528, 3.199453665659978, 3.19527694943187, 3.0879980954495103, 3.203625262438596, 3.1513454285296767, 3.1365202442630307, 3.1040016452034753, 3.1170750162103675, 3.121566424003014, 3.1636714987702423, 3.1189229488372803, 3.1287102646880096, 3.1511111652458106, 3.1628937131755954, 3.2058639893165, 3.2673832034016703, 3.1943222795213972, 3.0367659422067494],
        'accuracy': [0.02129121, 0.037087914, 0.06318682, 0.067307696, 0.089717045, 0.10370879, 0.09203297, 0.10645604, 0.10233516, 0.10164835, 0.14423077, 0.115384616, 0.13392857, 0.14423077, 0.15247253, 0.14491758, 0.14629121, 0.15453297, 0.16483517, 0.15041208, 0.13873626, 0.14423077, 0.19162089, 0.15453297, 0.17307693, 0.15247253, 0.16414835, 0.1668956, 0.17445055, 0.16002747, 0.17032968, 0.19093406, 0.18887363, 0.15865384, 0.19642857, 0.18956044, 0.19642857, 0.1826923, 0.25343406, 0.25343406, 0.260989, 0.27197802, 0.23763736, 0.23076923, 0.24038461, 0.25755495, 0.24725275, 0.27335164, 0.26442307, 0.2918956, 0.2451923, 0.25618133, 0.25892857, 0.27197802, 0.26923078, 0.25343406, 0.27335164, 0.27541208, 0.28296703, 0.2651099, 0.28434065, 0.27266484, 0.2596154, 0.2657967, 0.3083791, 0.2822802, 0.26991758, 0.26854396, 0.24793956, 0.28434065, 0.26854396, 0.29464287, 0.27953297, 0.27747253, 0.28296703, 0.28914836, 0.2747253, 0.2864011, 0.30494505, 0.29601648, 0.26167583, 0.29464287, 0.27815935, 0.2864011, 0.29326922, 0.2815934, 0.2967033, 0.29532966, 0.2760989, 0.2706044, 0.29258242, 0.2712912, 0.27747253, 0.27884614, 0.28708792, 0.2912088, 0.26442307, 0.26717034, 0.2864011, 0.29532966],
        'lr': [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0019999999, 0.0019999999, 0.0019999999, 0.0019999999, 0.0019999999, 0.0019999999, 0.0019999999, 0.0019999999, 0.0019999999, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001],
        'val_loss': [4.7450443456739535, 4.493839736868409, 4.201635150610886, 4.369762467553997, 4.050198370737129, 4.265352679624479, 4.053432905532128, 3.394700886025662, 3.2990336089712695, 3.2641264600400155, 3.2550013606262134, 3.2145898745794894, 3.225473241853589, 3.2013050844620725, 3.2171773894041644, 3.215544398364353, 3.198058796489303, 3.1818609567758145, 3.1721958723008434, 3.1857858909381287],
        'val_accuracy': [0.09277344, 0.11328125, 0.14550781, 0.13183594, 0.16796875, 0.13867188, 0.15136719, 0.2421875, 0.24707031, 0.26367188, 0.2607422, 0.26757812, 0.26367188, 0.2685547, 0.26660156, 0.28125, 0.27539062, 0.27734375, 0.2685547, 0.28320312]
        }
logs_500 = {'loss': [3.0734643019162693, 3.088763271059309, 3.112411399464031, 3.0085555066119185, 3.068098327615759,
          3.113976959343795, 3.0467579574375363, 3.083483722183731, 3.062722765482389, 3.0135254519326344,
          2.984087516973307, 2.9747128303234396, 3.058183206306709, 2.932997786081754, 3.0002262697115047,
          3.092358010155814, 2.9978386255411, 2.959159839284289, 2.937525670607011, 2.9864429431957205,
          2.9681676652405287, 3.0598258814968906, 2.9874074459075928, 2.9621385349022162, 2.9054518034169963,
          3.0665405281297455, 3.0093671148949928, 3.0119033724397095, 2.9758047245360992, 3.0800734601178013,
          3.015534390460004, 2.961805990764073, 2.9675705314992546, 2.8825716081556383, 2.9519218455304155,
          2.972303013225178, 2.974445852604541, 2.937679675909189, 3.0292983736310686, 3.00701789017562,
          3.0023477575281166, 2.952736270296705, 3.0492371386224097, 2.9038649705740123, 2.9371652341150973,
          3.009474303696182, 3.0108119524442234, 2.9696859475020525, 3.0321161642179386, 2.9775903460743662,
          2.982643309530321, 2.9797268359215705, 3.101435582716386, 3.031085109972692, 3.017805744454935,
          3.0297619903480615, 3.0409610847850423, 3.0647116205194496, 3.055085716666756, 3.0235005879140164,
          3.0341285362348454, 2.9706731890584086, 2.9823909473943186, 3.0395489734607737, 3.064679654089959,
          3.0526511721558625, 2.9905839359367286, 2.9098351709135284, 2.9122800643627462, 2.9852860904001925,
          2.9604295741070756, 2.998268874136956, 3.036122052224128, 3.056150651240087, 2.9585191758124383,
          2.99871152311891, 2.913769856914059, 2.945843532845214, 3.058030631516006, 2.9851567326011237,
          2.966944615919511, 2.9871228291438174, 3.0108372195736393, 3.0114655743588457, 3.0421440260750905,
          3.008960414718796, 2.9741995478724386, 3.094751549291087, 3.016133181341402, 2.9239213715542802,
          2.955041696737101, 3.018504182060996, 2.9194568382514703, 3.014708930319482, 2.960116590772356,
          2.9666140393896416, 2.9981904737241973, 2.997834698184506, 2.871924481549106, 2.9574971264535255,
          3.0010509490966797, 2.95867574607933, 2.994784556902372, 2.879472629054562, 2.9814982257046543,
          2.997886285677061, 3.004609206220606, 2.976074926145784, 3.0450729026899235, 2.900044456943051,
          3.0319005093731723, 2.9848941079862823, 3.0509756109216712, 2.988724335209354, 2.935974736790081,
          2.946694172345675, 2.96565880880251, 3.0292307308741977, 2.8928646871021817, 3.0089321372273203,
          2.9775574534803955, 3.0603953036633165, 3.050670629019266, 3.0115307111006517, 2.940860947409829,
          2.871038863947103, 3.0235563529716742, 2.9931224165381964, 3.060078412621886, 2.951040582342462,
          2.9964074538304257, 2.974494535844405, 2.968004577762478, 2.9948008270054074, 3.0030950635344116,
          2.986017182633117, 2.957533896624387, 2.9688070161002025, 3.012407653934353, 2.920648014152443,
          2.9695816537836097, 2.925335157048571, 3.002821738903339, 2.8673329759430097, 3.0215393632322876,
          2.9222972222736905, 2.931486209670266, 2.9544801987134495, 3.001553793529888, 2.930801001223889,
          3.042296207868136, 2.950998219814929, 2.9711107945704196, 3.0442336286817278, 3.0203390828855743,
          2.9519916406044593, 2.954967485679375, 3.0373434758448337, 2.910773592990833, 2.9473262435787326,
          2.9401562213897705, 2.9407534494504826, 2.969746830699208, 2.9653534574823066, 3.0175907638046766,
          2.9448164845560934, 3.0487443811290866, 2.977213387960916, 2.9382380417415073, 2.9857330374665314,
          2.907771665971358, 3.0256212818753587, 2.9647797411614722, 2.9588147532808913, 2.9238447776207557,
          3.001732003557813, 2.987634166256412, 2.957733521094689, 2.9715239739680026, 2.992297057267074,
          3.0099148240468905, 2.985147421176617, 2.968784143636515, 2.977647608453101, 3.012572636971107,
          2.975458187061352, 2.9975378670535244, 2.898676552615323, 2.9240365421379004, 3.088113153373802,
          3.057029894420079, 3.0130338826022305, 2.8935223568926802, 3.0178428167825215, 2.9897327161097262,
          2.980335443884462, 2.9237323776706234, 3.0245380113413045, 3.0043947185788835, 2.955100960783906,
          2.988636381023533, 3.1737499210860705, 2.9439212411314575, 3.0202221818022674, 3.0173424836043474,
          2.928275868132874, 3.016803298677717, 2.8825402587324707, 2.897348853257986, 2.9946670558426405,
          2.962984242282071, 2.9664004805323843, 2.9817125509073445, 2.9730819241031186, 3.0358774950216105,
          3.006959189425458, 2.940220819724785, 2.944498756429651, 3.0311763889186984, 2.975409358412355,
          2.9156069048158417, 2.977109733518663, 2.9625327036930966, 3.0960733078338287, 2.9327622010157657,
          2.974684613091605, 3.0275059951530707, 2.950158478139521, 2.9620069684563104, 2.9642696721213206,
          2.9886403476798926, 2.8866120969856177, 2.9886628559657504, 2.875350838179117, 2.966376569244888,
          2.9063780569768216, 2.943469728742327, 2.891067536322625, 2.9914836097549604, 2.9378868223546624,
          2.9076054856017395, 2.9064238346540012, 3.0022248986003164, 2.9584155698398966, 2.9444377487832374,
          2.980904398383675, 2.96540834746518, 2.9337156460835385, 2.9433570484538656, 2.974908545777038,
          3.009544183919718, 2.985234027380472, 2.89551596065144, 3.051938457803412, 2.9464588047383904,
          2.972139989936745, 2.9042368066179884, 2.9347416437589207, 2.926040130657154, 2.892503334925725,
          2.9376203437427897, 2.9236496700035346, 2.985761563856523, 2.922105781324617, 2.9861342959351593,
          2.9861276149749756, 2.96023546732389, 2.923522778919765, 2.9360583273919074, 3.005492021749308,
          3.060339872653668, 2.948459428734433, 2.941625862331181, 2.965738268998953, 2.964388593212589,
          2.974367086703961, 2.992011987246, 2.9332632153898803, 2.9857324977497477, 2.9850411886697286,
          2.987282640331394, 2.995550187079461, 2.8969865172773925, 2.991029503581288, 2.9879009487864736,
          2.966101738122793, 2.92656688113789, 3.0515513446304823, 3.0187755128839515, 3.034458113240672,
          2.9410435812813893, 2.9586909170989153, 2.8877046579843038, 2.9412384766798754, 2.9616843999087155,
          2.946217927303943, 2.985898520920303, 2.953822454253396, 2.969972272495647, 2.9463047431065488,
          2.9410562017461754, 2.9123152062133117, 2.914740334500323, 2.89446521460355, 2.972452019597148,
          3.0123575794827806, 2.985061454248952, 3.0524090908385895, 2.908787135239486, 2.9313518568709656,
          3.0123961050431807, 2.8863622487246334, 2.9386429013786737, 2.938002114767557, 3.0299815188397416,
          2.92631002834865, 2.948887764752566, 2.979577051414238, 2.9263646484731316, 2.955245940239875,
          3.0087239008683424, 2.9917419782051673, 2.921785511813321, 2.9484599291623295, 2.97439528297592,
          2.9306843333191925, 2.9249611713074066, 2.9910349557687947, 2.9356037902308034, 2.9930066721779958,
          2.966363068465348, 2.980078313376877, 2.990946155328017, 2.97179813175411, 3.0561299664633617,
          2.955730139554202, 2.9776264206393734, 3.038770337681194, 2.9267257307911967, 2.93383014857114,
          2.9610470186055324, 2.970422964829665, 2.992802775823153, 3.04418315075256, 2.9433629303188114,
          2.981763782082023, 2.918781008039202, 2.9221015841096314, 2.9878216418591172, 2.9576534504418843,
          2.956196177136767, 2.9994191245718316, 2.9448991476834476, 3.0214000717624203, 3.024209894976773,
          2.9736156922120314, 3.0168409819131368, 2.9193828394124797, 2.945813632273412, 2.9653338966788825,
          2.9921906832810286, 2.958506382428683, 2.9798343653207295, 3.0400697189372976, 2.9384559641827592,
          2.9911174210873277, 2.9977541074648006, 2.9212148241944367, 2.9303747166644087, 2.9798368831257243,
          2.957128848348345, 2.967025088739919, 2.8963480362525353, 3.0099835212414083, 2.970441760597648,
          2.89478600025177, 2.8366425875779036, 2.972086630024753, 2.952197098469996, 3.0133038457933363,
          2.9504614112141367, 2.8965220202456465, 2.9585407199440423, 3.02167848440317, 2.935420490883209,
          2.9600816637605103, 3.002090925698752, 2.9558245166317447, 2.9467528757158217, 2.971336073927827,
          2.9937098733671417, 2.911658622406341, 2.9401140868008793, 3.053003130378304, 2.944668481638143,
          2.906208012130234, 2.945334565508497, 2.8671374844980764, 3.0159195753244252, 2.9272425331912197],
 'acc': [0.29945055, 0.2918956, 0.28983516, 0.29532966, 0.30631867, 0.29326922, 0.2815934, 0.31043956, 0.2864011,
         0.30769232, 0.31868133, 0.33791208, 0.28914836, 0.31456044, 0.3125, 0.2809066, 0.31936812, 0.31318682,
         0.3083791, 0.30631867, 0.3221154, 0.301511, 0.2973901, 0.3166209, 0.33035713, 0.31318682, 0.29395604, 0.3125,
         0.31593406, 0.29945055, 0.3125, 0.32692307, 0.3221154, 0.32967034, 0.3392857, 0.32074177, 0.31456044,
         0.31936812, 0.30013737, 0.31730768, 0.30013737, 0.323489, 0.3125, 0.32486263, 0.3070055, 0.29945055,
         0.32554945, 0.3221154, 0.29532966, 0.32417583, 0.32142857, 0.32142857, 0.27953297, 0.31112638, 0.31557092,
         0.31181318, 0.2857143, 0.28365386, 0.29464287, 0.30906594, 0.31112638, 0.29395604, 0.30563188, 0.30357143,
         0.30631867, 0.30769232, 0.3221154, 0.3331044, 0.29532966, 0.31730768, 0.31456044, 0.3070055, 0.3228022,
         0.30013737, 0.3337912, 0.30425823, 0.32692307, 0.33104396, 0.30563188, 0.32074177, 0.31868133, 0.30769232,
         0.31730768, 0.32074177, 0.29876372, 0.31456044, 0.30494505, 0.2912088, 0.31524727, 0.32486263, 0.33447802,
         0.30906594, 0.32417583, 0.30975273, 0.32417583, 0.33585164, 0.31043956, 0.29326922, 0.33104396, 0.32692307,
         0.31868133, 0.3125, 0.3276099, 0.34546703, 0.31043956, 0.3028846, 0.32142857, 0.30563188, 0.31387362,
         0.32554945, 0.31318682, 0.29464287, 0.30013737, 0.32692307, 0.3337912, 0.32417583, 0.30082417, 0.29876372,
         0.32486263, 0.31868133, 0.29395604, 0.30906594, 0.3179945, 0.30563188, 0.32692307, 0.32623628, 0.29876372,
         0.30906594, 0.28502747, 0.31318682, 0.32623628, 0.32692307, 0.32692307, 0.31318682, 0.31112638, 0.3021978,
         0.3228022, 0.3228022, 0.31387362, 0.31318682, 0.3385989, 0.3276099, 0.323489, 0.33035713, 0.31456044,
         0.31868133, 0.32623628, 0.33516484, 0.3021978, 0.32074177, 0.29876372, 0.34684065, 0.3228022, 0.31112638,
         0.3166209, 0.33653846, 0.32074177, 0.31387362, 0.3276099, 0.30563188, 0.32898352, 0.3125, 0.34203297,
         0.3282967, 0.31181318, 0.33035713, 0.3179945, 0.30563188, 0.3228022, 0.30769232, 0.3331044, 0.30975273,
         0.33997253, 0.31112638, 0.3282967, 0.31387362, 0.31387362, 0.3228022, 0.33241758, 0.3125, 0.30934256,
         0.31524727, 0.30975273, 0.31387362, 0.31318682, 0.31593406, 0.30769232, 0.3282967, 0.33104396, 0.29258242,
         0.3070055, 0.29807693, 0.31456044, 0.28983516, 0.3083791, 0.3070055, 0.31181318, 0.31387362, 0.30563188,
         0.32074177, 0.32005495, 0.2657967, 0.3221154, 0.3125, 0.31868133, 0.3228022, 0.30357143, 0.33997253,
         0.32074177, 0.31318682, 0.31936812, 0.3083791, 0.32142857, 0.32142857, 0.3021978, 0.3125, 0.33104396,
         0.33653846, 0.31043956, 0.30082417, 0.33997253, 0.31456044, 0.31524727, 0.28983516, 0.32898352, 0.29807693,
         0.31318682, 0.32142857, 0.32623628, 0.33104396, 0.2973901, 0.32074177, 0.31387362, 0.3482143, 0.29945055,
         0.3276099, 0.33653846, 0.33173078, 0.30906594, 0.32898352, 0.32967034, 0.3372253, 0.31593406, 0.3337912,
         0.31936812, 0.31456044, 0.30631867, 0.31868133, 0.32486263, 0.31318682, 0.32417583, 0.31456044, 0.3276099,
         0.30425823, 0.31043956, 0.3028846, 0.3440934, 0.32692307, 0.32692307, 0.32967034, 0.3385989, 0.32554945,
         0.31387362, 0.32623628, 0.30975273, 0.32623628, 0.323489, 0.32554945, 0.3179945, 0.31043956, 0.2918956,
         0.3231834, 0.31043956, 0.29807693, 0.3166209, 0.31387362, 0.32074177, 0.3282967, 0.30769232, 0.30631867,
         0.30631867, 0.31112638, 0.31593406, 0.31181318, 0.29876372, 0.33104396, 0.32554945, 0.3125, 0.3070055,
         0.29395604, 0.3337912, 0.32005495, 0.34752747, 0.323489, 0.31936812, 0.32142857, 0.3221154, 0.33516484,
         0.31593406, 0.32142857, 0.31868133, 0.33653846, 0.31936812, 0.323489, 0.3331044, 0.31456044, 0.32142857,
         0.3028846, 0.33791208, 0.3276099, 0.3166209, 0.33585164, 0.33653846, 0.31387362, 0.3179945, 0.33516484,
         0.31730768, 0.33104396, 0.31936812, 0.3282967, 0.29258242, 0.30975273, 0.32417583, 0.30013737, 0.3179945,
         0.32417583, 0.32486263, 0.301511, 0.31456044, 0.31318682, 0.31181318, 0.30975273, 0.31043956, 0.3028846,
         0.30494505, 0.33104396, 0.31936812, 0.29945055, 0.323489, 0.32142857, 0.31211072, 0.3179945, 0.3179945,
         0.3166209, 0.33104396, 0.31593406, 0.3385989, 0.33241758, 0.30975273, 0.32554945, 0.32692307, 0.31043956,
         0.31181318, 0.30357143, 0.30357143, 0.32005495, 0.30631867, 0.33035713, 0.30975273, 0.32005495, 0.3021978,
         0.30906594, 0.3221154, 0.3125, 0.3228022, 0.31387362, 0.31593406, 0.32142857, 0.3221154, 0.30769232,
         0.30425823, 0.33516484, 0.33516484, 0.30082417, 0.31181318, 0.3331044, 0.3482143, 0.31456044, 0.323489,
         0.32692307, 0.31868133, 0.33997253, 0.33173078, 0.2973901, 0.32967034, 0.32486263, 0.3028846, 0.3070055,
         0.31936812, 0.31936812, 0.31181318, 0.31524727, 0.3331044, 0.30494505, 0.31936812, 0.32554945, 0.30769232,
         0.34752747, 0.31181318, 0.3282967],
 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00020000001, 0.00020000001, 0.00020000001,
        0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04,
        1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04],
 'val_loss': [3.284863552175816, 3.2411238788493044, 3.168716784766213, 3.1765211697996905, 3.1919009148830284,
              3.138180303613633, 3.15725872100256, 3.1418249602730537, 3.1275969927077742, 3.1506803958308742,
              3.157884600321075, 3.16419106921708, 3.151591854766935, 3.1626536970591066, 3.1440570582485563,
              3.1600948117700254, 3.1446099021149547, 3.1729763661797405, 3.156341208426539, 3.1455028912335132,
              3.162032891949071, 3.1434211661266716, 3.159031586158966, 3.1221741066210678, 3.1248985120633654,
              3.1585011637393903, 3.1527425888137617, 3.137907464139427, 3.154925485425529, 3.145199436991443,
              3.1276443406857837, 3.1636780279392442, 3.1379479297277624, 3.142806538969053, 3.1439479947861217,
              3.131476814632786, 3.12896344277121, 3.165762292033848, 3.1444494109656884, 3.1283268801154653,
              3.1480383379738495, 3.146715717469181, 3.1422044296991687, 3.147985859281698, 3.151930594911624,
              3.1471913821348902, 3.15019496698671, 3.1629822134459538, 3.145500582035445, 3.1553738992938065,
              3.156174625966871, 3.1310006188138506, 3.1164300009120467, 3.128739134126879, 3.1308422428598783,
              3.1360767240001146, 3.1185947554391538, 3.1491000680683783, 3.1592963302552786, 3.153671790527802,
              3.133212521665627, 3.1446731733886164, 3.0863865617998725, 3.1494925677002326, 3.1579601634598213,
              3.1320192689913995, 3.1228009653710638, 3.136025550588763, 3.146484093225097, 3.114237524706988,
              3.1150643325102734, 3.113803052669136, 3.126002839893222, 3.13137951188221, 3.1382698801566433,
              3.1450367157520356, 3.1257179496447574, 3.1303607298581153, 3.1093863666790393, 3.1133012861544627],
 'val_acc': [0.28027344, 0.2705078, 0.2919922, 0.29785156, 0.29882812, 0.2861328, 0.3046875, 0.3046875, 0.31054688,
             0.2998047, 0.2939453, 0.29492188, 0.29296875, 0.29785156, 0.27929688, 0.28808594, 0.28710938, 0.2861328,
             0.29882812, 0.2861328, 0.28808594, 0.2939453, 0.3046875, 0.29101562, 0.30078125, 0.29003906, 0.2998047,
             0.29296875, 0.29492188, 0.3203125, 0.30371094, 0.30273438, 0.28515625, 0.2890625, 0.27929688, 0.30371094,
             0.29785156, 0.28808594, 0.28710938, 0.3046875, 0.29589844, 0.3017578, 0.3046875, 0.296875, 0.29589844,
             0.29296875, 0.296875, 0.28125, 0.2939453, 0.3095703, 0.29589844, 0.3017578, 0.31054688, 0.2890625,
             0.30371094, 0.29101562, 0.3125, 0.29882812, 0.30371094, 0.29492188, 0.296875, 0.29296875, 0.30664062,
             0.29785156, 0.3017578, 0.2998047, 0.30859375, 0.30078125, 0.3017578, 0.30078125, 0.29785156, 0.30371094,
             0.29785156, 0.29492188, 0.30273438, 0.29296875, 0.30566406, 0.29003906, 0.29882812, 0.29492188]
}

def train():
    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
    epoch = 500
    batch_size = Datasets_loader.batch_size
    learning_rate = 0.001
    round_epoch = round(epoch/100)

    filepath = "D:\BaiduNetdiskDownload\dataset_release/release_data"

    train_data, train_num = load_data_by_keras(filepath, type="train", shuffle=True)
    val_data, val_num = load_data_by_keras(filepath, type="val", shuffle=True)

    base_model = keras.applications.vgg16.VGG16(weights =None,
                                                include_top=False,
                                                input_tensor=None,
                                                pooling=None,
                                                input_shape=[224, 224, 3])
    base_model.trainable = False

    model = ClassificationLayer(base_model, 208)

    model.build((batch_size, 224, 224, 3))
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,
                                         beta_1=0.9, beta_2=0.999,
                                         epsilon=1e-7)
    loss_object = tf.keras.losses.CategoricalCrossentropy()
    train_loss = tf.keras.metrics.Mean(name="loss", dtype=tf.float32)
    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')

    model.compile(loss=loss_object,
                  optimizer=optimizer,
                  metrics=['accuracy'])
    print(model.metrics_names)
    model.load_weights("models/VGG16/weights.100-0.30-3.04.h5")
    # model = keras.models.load_model('models/VGG16.h5')
    model.summary()
    # plot_model(model, to_file='model.png')
    steps_per_epoch = round(train_num / (batch_size* (epoch/round_epoch)))
    val_freq  = 5
    validation_steps = val_num / (batch_size*(epoch/round_epoch)/val_freq)
    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,
                                  patience=5, min_lr=0.0001)

    model_name = "VGG16"

    log_dir = os.getcwd()
    log_dir = os.path.join(log_dir, "logs")
    log_dir = os.path.join(log_dir, model_name)
    print("model logs at {}".format(log_dir))
    tensorboard = TensorBoard(log_dir=log_dir)
    checkpath = "./models/VGG16/weights.{epoch:02d}-{acc:.2f}-{loss:.2f}.h5"
    checkpoint = ModelCheckpoint(checkpath,
                                 monitor='loss',
                                 verbose=1,
                                 save_best_only=True,
                                 save_freq='epoch',
                                 mode='min',
                                 save_weights_only=True)
    history = model.fit_generator(generator=train_data,
                                  epochs=epoch,
                                  verbose=1,
                                  steps_per_epoch=steps_per_epoch,
                                  validation_data=val_data,
                                  validation_steps=validation_steps,
                                  validation_freq= val_freq,
                                  callbacks=[tensorboard, reduce_lr,checkpoint],
                                  initial_epoch=100
                                  )
    print('history dict:', history.history)

    localtime = time.strftime("%y-%m-%d-%H:%M:%S", time.localtime())
    print(localtime)

    model.save("./models/VGG16.h5")
    print("Success Save Model!")

    acc = history.history['acc']
    val_acc = history.history['val_acc']

    loss = history.history['loss']
    val_loss = history.history['val_loss']

    plt.figure(figsize=(8, 8))
    plt.subplot(2, 1, 1)
    plt.plot(acc, label='Training Accuracy')
    plt.plot(val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.ylabel('Accuracy')
    plt.ylim([min(plt.ylim()), 1])
    plt.title('Training and Validation Accuracy')

    plt.subplot(2, 1, 2)
    plt.plot(loss, label='Training Loss')
    plt.plot(val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.ylabel('Cross Entropy')
    plt.ylim([0, 1.0])
    plt.title('Training and Validation Loss')
    plt.xlabel('epoch')
    plt.savefig("./models/DenseNet_in_{}".format(localtime))
    plt.show()

    acc_text = np.array(acc)
    val_acc_text = np.array(val_acc)
    loss_text = np.array(loss)
    val_loss_text = np.array(val_loss)
    np.savetxt("./models/denseNet_acc.txt", acc_text)
    np.savetxt("./models/denseNet_val_acc.txt", val_acc_text)
    np.savetxt("./models/denseNet_loss.txt", loss_text)
    np.savetxt("./models/denseNet_val_loss.txt", val_loss_text)

def draw_acc_loss():
    loss = logs['loss']
    val_loss = logs['val_loss']

    acc = logs['accuracy']
    val_acc = logs['val_accuracy']

    plt.figure(figsize=(8, 8))
    plt.subplot(2, 1, 1)
    plt.plot(acc, label='Training Accuracy')
    plt.plot(val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.ylabel('Accuracy')
    plt.ylim([min(plt.ylim()), 1])
    plt.title('Training and Validation Accuracy')

    plt.subplot(2, 1, 2)
    plt.plot(loss, label='Training Loss')
    plt.plot(val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.ylabel('Cross Entropy')
    plt.title('Training and Validation Loss')
    plt.xlabel('epoch')
    plt.show()

    # acc_text = np.array(acc)
    # val_acc_text = np.array(val_acc)
    # loss_text = np.array(loss)
    # val_loss_text = np.array(val_loss)
    # np.savetxt("./models/denseNet_acc.txt", acc_text)
    # np.savetxt("./models/denseNet_val_acc.txt", val_acc_text)
    # np.savetxt("./models/denseNet_loss.txt", loss_text)
    # np.savetxt("./models/denseNet_val_loss.txt", val_loss_text)

def predicts(imagepath):
    model = keras.models.load_model("models/VGG16.h5")
    model.summary()
    xlspath = "D:\BaiduNetdiskDownload\dataset_release/release_data"
    id, ChineseName, EnglishName = read_chinesefoodnet_from_xlsx(xlspath)
    img = keras_image.load_img(imagepath, color_mode='rgb', target_size=[224,224])
    x = keras_image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)

    preds = model.predict(x)
    results = []
    # 将结果解码为元组列表 (class, description, probability)
    for pred in preds:
        top_indices = pred.argsort()[-5:][::-1]
        print(top_indices)
        result = [tuple(ChineseName[i])+(pred[i],) for i in top_indices]
        # for i in top_indices:
        #     print(i)
        #     print("pred:{}".format(pred[i]))
        # print("start next pred")
        # result = [tuple(ChineseName[str(i)]) + (pred[i],) for i in top_indices]
        result.sort(key=lambda x: x[1], reverse=True)
        results.append(result)
    print(results)
    # 示例如下：
    # Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]
    return results

if __name__ == "__main__":
    # draw_acc_loss()
    # imagepath = 'D:\BaiduNetdiskDownload\dataset_release/release_data/val/000/000003.jpg'
    # predicts(imagepath)
    train()


